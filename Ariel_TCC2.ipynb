{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OmO8ijFsBzc9",
    "outputId": "89b41e5f-10af-402a-c17d-f0ce8b4288b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.82-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow==2.15) (1.16.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow==2.15) (65.5.0)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow==2.15) (4.12.2)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from tensorflow==2.15) (24.1)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: pytz, libclang, flatbuffers, wrapt, wheel, werkzeug, tzdata, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, pandas, opt-einsum, opencv-python, ml-dtypes, h5py, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.25 gast-0.5.4 google-auth-2.30.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-2.15.0 libclang-18.1.1 markdown-3.6 ml-dtypes-0.2.0 numpy-1.26.4 oauthlib-3.2.2 opencv-python-4.10.0.82 opt-einsum-3.3.0 pandas-2.2.2 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pytz-2024.1 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 tzdata-2024.1 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15 opencv-python pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OmO8ijFsBzc9",
    "outputId": "89b41e5f-10af-402a-c17d-f0ce8b4288b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ml-dtypes 0.4.0\n",
      "Uninstalling ml-dtypes-0.4.0:\n",
      "  Successfully uninstalled ml-dtypes-0.4.0\n",
      "Collecting ml-dtypes\n",
      "  Using cached ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Requirement already satisfied: numpy>1.20 in ./venv/lib/python3.10/site-packages (from ml-dtypes) (1.26.4)\n",
      "Installing collected packages: ml-dtypes\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ml-dtypes-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=9.1 in ./venv/lib/python3.10/site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: packaging>=21 in ./venv/lib/python3.10/site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: scipy>=1.9 in ./venv/lib/python3.10/site-packages (from scikit-image) (1.13.1)\n",
      "Collecting networkx>=2.8\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy-loader>=0.4\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting imageio>=2.33\n",
      "  Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n",
      "Collecting tifffile>=2022.8.12\n",
      "  Using cached tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "Installing collected packages: tifffile, networkx, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.34.1 lazy-loader-0.4 networkx-3.3 scikit-image-0.23.2 tifffile-2024.5.22\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall ml-dtypes --y\n",
    "!pip install ml-dtypes\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OmO8ijFsBzc9",
    "outputId": "89b41e5f-10af-402a-c17d-f0ce8b4288b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aBG_WT2YU7A2"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-Z8KINaj1Vq",
    "outputId": "55d905dc-828b-4f6f-8450-86709620f6d7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mr9V4UNZU9uK",
    "outputId": "dabe7318-de43-4cd6-e796-0fa74a2d7b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/okankop/vidaug\n",
      "  Cloning https://github.com/okankop/vidaug to /tmp/pip-req-build-xwgtpuwc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/okankop/vidaug /tmp/pip-req-build-xwgtpuwc\n",
      "  Resolved https://github.com/okankop/vidaug to commit 1c1ddf2640fe4a9171267d64ae5e3bd70c24d54a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !unzip videos.zip\n",
    "# df_train = pd.read_csv('/content/drive/MyDrive/TCC2/train_info.csv', index_col=0)\n",
    "# df_train\n",
    "\n",
    "!pip install mediapipe -q\n",
    "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n",
    "!pip install git+https://github.com/okankop/vidaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ut8SkrMjKqPP",
    "outputId": "c0719cf5-47c9-4fd4-86d0-4531613954a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempo.mp4\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'videos'\n",
    "files = os.listdir(folder_path)\n",
    "files.sort()\n",
    "print(files[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wFZw4EhoXA9K"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'palavra': list(map(lambda x: x.replace('.mp4', ''), files))}).to_csv('palavras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "auAbUhQiU_3D"
   },
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_landmarks_new_image(rgb_image, detection_result, stick=False):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "  if stick:\n",
    "    image_height, image_width, _ = rgb_image.shape\n",
    "    annotated_image = np.zeros((image_height, image_width, 3)).astype('uint8')\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNppwKaiVV5-",
    "outputId": "fdb85f91-125e-4b05-fe70-db9a3eadc150"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718223033.466950   10065 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1718223033.471999   10588 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.1.0-devel), renderer: Radeon RX 590 Series (radeonsi, polaris10, LLVM 17.0.6, DRM 3.42, 5.15.0-107-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1718223033.560122   10589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1718223033.630425   10604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dos dados: (228, 90, 128, 128)\n",
      "Shape dos dados preprocces MediaPipe: (228, 90, 128, 128)\n",
      "Shape dos rótulos: (228,)\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "#inicializacao do framework mediapipe para fazer a extracao de caracteristicas do corpo humano\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "\n",
    "max_frames = 90\n",
    "def preprocess_image(image, target_size=None):\n",
    "    \"\"\"\n",
    "    Pre processamento da imagem onde é responsavel pela transformação de\n",
    "    RBG2Gray e redimensionar o tamanho da imagem\n",
    "    \"\"\"\n",
    "    resized_image = cv2.resize(image, target_size) if target_size else image\n",
    "    resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def read_videos(folder_path, target_size=None):\n",
    "    \"\"\"\n",
    "    Função que é responsável pela leitura dos videos usando a lib do openCV e\n",
    "    armazena cada frame do video em um array numpy junto com a extração de\n",
    "    caracteristicas do corpo humano retornado do framework mediapipe e seu\n",
    "    identificador\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    mp_data = []\n",
    "    labels = []\n",
    "    files = os.listdir(folder_path)\n",
    "    files.sort()\n",
    "\n",
    "    if os.path.isfile('ariel_data.npy'):\n",
    "      with open('ariel_data.npy', 'rb') as f:\n",
    "          data = np.load(f)\n",
    "          mp_data = np.load(f)\n",
    "          labels = np.load(f)\n",
    "    else:\n",
    "      for idx, filename in enumerate(files):\n",
    "          if filename.endswith('.mp4') or filename.endswith('.avi'):\n",
    "              video_path = os.path.join(folder_path, filename)\n",
    "\n",
    "              cap = cv2.VideoCapture(video_path)\n",
    "              frames = []\n",
    "              mp_frames = []\n",
    "              while cap.isOpened():\n",
    "                  ret, frame = cap.read()\n",
    "                  if not ret:\n",
    "                      break\n",
    "\n",
    "                  processed_frame = preprocess_image(frame, target_size)\n",
    "                  #frame pre processado\n",
    "                  frames.append(processed_frame)\n",
    "                  image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "                  detection_result = detector.detect(image)\n",
    "                  #deteccao com o framework mediapipe se existe um corpo humano\n",
    "                  #no frame\n",
    "                  if detection_result.pose_world_landmarks:\n",
    "                    annotated_image = draw_landmarks_new_image(cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB), detection_result)\n",
    "                    annotated_image = preprocess_image(annotated_image, target_size)\n",
    "                    mp_frames.append(annotated_image)\n",
    "                  else:\n",
    "                    mp_frames.append(processed_frame)\n",
    "\n",
    "\n",
    "              cap.release()\n",
    "              while len(frames) < max_frames:\n",
    "                  frames.append(np.zeros_like(processed_frame))\n",
    "              while len(mp_frames) < max_frames:\n",
    "                  mp_frames.append(np.zeros_like(processed_frame))\n",
    "              #normalizacao do frames contidos dentro da estrutura, foi pré\n",
    "              #definido que cada video terá 90 frames, preenchido com zeros\n",
    "              #caso nao atinja os 90 frames\n",
    "              data.append(frames)\n",
    "              mp_data.append(mp_frames)\n",
    "              labels.append(idx)\n",
    "      with open('ariel_data.npy', 'wb') as f:\n",
    "        np.save(f, np.array(data))\n",
    "        np.save(f, np.array(mp_data))\n",
    "        np.save(f, np.array(labels))\n",
    "        #salva e recupera o array numpy pois o processamentos de videos e lento\n",
    "\n",
    "    return data, mp_data, labels\n",
    "\n",
    "target_size = (128, 128)\n",
    "\n",
    "data, mp_data, labels = read_videos(folder_path, target_size)\n",
    "\n",
    "print(\"Shape dos dados:\", data.shape)\n",
    "print(\"Shape dos dados preprocces MediaPipe:\", mp_data.shape)\n",
    "print(\"Shape dos rótulos:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUfPnlN1fHBv",
    "outputId": "eefd8733-f032-4a95-9fc7-004d6df20654"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ballke/Documentos/TCC2/venv/lib/python3.10/site-packages/keras/src/preprocessing/image.py:766: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (228, 90, 128, 128) (128 channels).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "# X_reshaped = X.reshape(-1, 128, 128, 1)\n",
    "\n",
    "X_mp = mp_data\n",
    "# X_mp = X.reshape(-1, 32, 32, 1)\n",
    "\n",
    "y_train_repeated = np.repeat(labels, max_frames)\n",
    "Y_reshaped = to_categorical(y_train_repeated)\n",
    "Y = to_categorical(labels)\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "#gerador de variações para o video ja que não possuimos um dataset amplo p treino\n",
    "# train_generator = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     brightness_range=(0.3, 1.0),\n",
    "#     # horizontal_flip=True,\n",
    "#     # vertical_flip=True,\n",
    "#     validation_split=0.2).flow(X, Y, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train_generator_mp = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     brightness_range=(0.3, 1.0),\n",
    "#     # horizontal_flip=True,\n",
    "#     # vertical_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "#     validation_split=0.2).flow(X_mp, Y, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train_generator_reshaped = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     brightness_range=(0.3, 1.0),\n",
    "#     # horizontal_flip=True,\n",
    "#     # vertical_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "#     validation_split=0.2).flow(X_reshaped, Y_reshaped, batch_size=BATCH_SIZE)\n",
    "\n",
    "# X.shape, train_generator, X_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LFU27WLbLXnM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1140, 90, 128, 128, 1), (1140,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vidaug import augmentors as va\n",
    "\n",
    "sometimes = lambda aug: va.Sometimes(0.5, aug) # Used to apply augmentor with 50% probability\n",
    "seq = va.Sequential([\n",
    "    sometimes(va.RandomCrop(size=(128, 128))),\n",
    "    # sometimes(va.RandomRotate(degrees=10)),\n",
    "    # sometimes(va.VerticalFlip()),\n",
    "    sometimes(va.HorizontalFlip()),\n",
    "    sometimes(va.GaussianBlur(1.5))\n",
    "])\n",
    "\n",
    "\n",
    "X_variation = np.empty((0, 90, 128, 128, 1))\n",
    "Y_variation = np.empty(0, dtype=int)\n",
    "\n",
    "#os dados estao salvos pois gerar levam muito tempo e processamento para terminar\n",
    "if os.path.isfile('ariel_data_variation.npy'):\n",
    "  with open('ariel_data_variation.npy', 'rb') as f:\n",
    "      X_variation = np.load(f)\n",
    "      Y_variation = np.load(f)\n",
    "else:\n",
    "    for idx, video in enumerate(X):\n",
    "      video = np.expand_dims(video, axis=-1)\n",
    "      for _ in range(5):\n",
    "          # 'video' should be either a list of images from type of numpy array or PIL images\n",
    "          video_aug = np.array(seq(video))\n",
    "          X_variation = np.concatenate((X_variation, [video_aug]), axis=0)\n",
    "          Y_variation = np.concatenate((Y_variation, [idx]), axis=0)\n",
    "          # for v in video_aug:\n",
    "          #   cv2_imshow(v)\n",
    "      print(f'variacoes de palavras processadas: {round((idx/len(files))*100, 2)}%')\n",
    "    with open('ariel_data_variation.npy', 'wb') as f:\n",
    "        np.save(f, X_variation)\n",
    "        np.save(f, Y_variation)\n",
    "X_variation.shape, Y_variation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "orDi6VTHB29K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 90, 128, 128, 8)   80        \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 90, 128, 128, 8)   32        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 90, 64, 64, 8)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 90, 64, 64, 16)    1168      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 90, 64, 64, 16)    64        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 90, 32, 32, 16)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 90, 32, 32, 32)    4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 90, 32, 32, 32)    128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 90, 16, 16, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 90, 16, 16, 64)    18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 90, 16, 16, 64)    256       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 90, 8, 8, 64)      0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_12 (TimeD  (None, 90, 8, 8, 128)     73856     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_13 (TimeD  (None, 90, 8, 8, 128)     512       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_14 (TimeD  (None, 90, 4, 4, 128)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_15 (TimeD  (None, 90, 4, 4, 4)       4612      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_16 (TimeD  (None, 90, 4, 4, 4)       16        \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_17 (TimeD  (None, 90, 2, 2, 4)       0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_18 (TimeD  (None, 90, 16)            0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 200)               93600     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25728     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 228)               29412     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252600 (986.72 KB)\n",
      "Trainable params: 252096 (984.75 KB)\n",
      "Non-trainable params: 504 (1.97 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (90, 128, 128, 1)\n",
    "model = Sequential()\n",
    "#rede resultante final, necessaria criar com filtros Conv2d em baixo numero para\n",
    "# que consigamos compilar com baixo poder computacional\n",
    "model.add(TimeDistributed(Conv2D(filters=8, kernel_size=3, activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(filters=16, kernel_size=3, activation='relu', padding='same'), input_shape=input_shape))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(filters=4, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# de maneira geral, colocar mais neuronios pra LSTM ao invés de camadas\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "# tem que ser mesmo tamanho do vocabulário\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "#adicionado clipnorm para resolver problema de explosao de gradiente\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy') # loss deve ser a crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LtWCeLI1fvLc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 228)\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:14:30.150739: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6723993600 exceeds 10% of free system memory.\n",
      "2024-06-12 17:14:35.822265: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\n",
      "2024-06-12 17:14:36.192727: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\n",
      "2024-06-12 17:14:36.883903: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 754974720 exceeds 10% of free system memory.\n",
      "2024-06-12 17:14:36.977037: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 754974720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 256s 7s/step - loss: 5.4656\n",
      "Epoch 2/300\n",
      "36/36 [==============================] - 252s 7s/step - loss: 5.4282\n",
      "Epoch 3/300\n",
      "36/36 [==============================] - 253s 7s/step - loss: 5.4152\n",
      "Epoch 4/300\n",
      "36/36 [==============================] - 253s 7s/step - loss: 5.3478\n",
      "Epoch 5/300\n",
      "36/36 [==============================] - 254s 7s/step - loss: 5.2033\n",
      "Epoch 6/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 5.0589\n",
      "Epoch 7/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.9323\n",
      "Epoch 8/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.8992\n",
      "Epoch 9/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.7548\n",
      "Epoch 10/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.6661\n",
      "Epoch 11/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.5899\n",
      "Epoch 12/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.5638\n",
      "Epoch 13/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.4348\n",
      "Epoch 14/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 4.3411\n",
      "Epoch 15/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.2476\n",
      "Epoch 16/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 4.1024\n",
      "Epoch 17/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 4.0204\n",
      "Epoch 18/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 3.8736\n",
      "Epoch 19/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 3.7977\n",
      "Epoch 20/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 3.7170\n",
      "Epoch 21/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 3.6423\n",
      "Epoch 22/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 3.5140\n",
      "Epoch 23/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 3.4199\n",
      "Epoch 24/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 3.3083\n",
      "Epoch 25/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 3.1908\n",
      "Epoch 26/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 3.0443\n",
      "Epoch 27/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.9334\n",
      "Epoch 28/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.8909\n",
      "Epoch 29/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 2.8194\n",
      "Epoch 30/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 2.6026\n",
      "Epoch 31/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 2.5890\n",
      "Epoch 32/300\n",
      "36/36 [==============================] - 255s 7s/step - loss: 2.4894\n",
      "Epoch 33/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.5223\n",
      "Epoch 34/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.3976\n",
      "Epoch 35/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.3252\n",
      "Epoch 36/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 2.1708\n",
      "Epoch 37/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 2.1923\n",
      "Epoch 38/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 2.0545\n",
      "Epoch 39/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 2.0173\n",
      "Epoch 40/300\n",
      "36/36 [==============================] - 261s 7s/step - loss: 1.8989\n",
      "Epoch 41/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 1.9474\n",
      "Epoch 42/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 1.7940\n",
      "Epoch 43/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.7535\n",
      "Epoch 44/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.7315\n",
      "Epoch 45/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.7247\n",
      "Epoch 46/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.6352\n",
      "Epoch 47/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.5512\n",
      "Epoch 48/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.4635\n",
      "Epoch 49/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.3538\n",
      "Epoch 50/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 1.4428\n",
      "Epoch 51/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.3257\n",
      "Epoch 52/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.2524\n",
      "Epoch 53/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 1.2546\n",
      "Epoch 54/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.1967\n",
      "Epoch 55/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 1.1690\n",
      "Epoch 56/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.1738\n",
      "Epoch 57/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.9917\n",
      "Epoch 58/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.0098\n",
      "Epoch 59/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 1.0152\n",
      "Epoch 60/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.9872\n",
      "Epoch 61/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.9984\n",
      "Epoch 62/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.9124\n",
      "Epoch 63/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8628\n",
      "Epoch 64/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8718\n",
      "Epoch 65/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.8022\n",
      "Epoch 66/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8422\n",
      "Epoch 67/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8315\n",
      "Epoch 68/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8306\n",
      "Epoch 69/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.7819\n",
      "Epoch 70/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8531\n",
      "Epoch 71/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.7121\n",
      "Epoch 72/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.8357\n",
      "Epoch 73/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.6099\n",
      "Epoch 74/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.7203\n",
      "Epoch 75/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.5905\n",
      "Epoch 76/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.6766\n",
      "Epoch 77/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.6997\n",
      "Epoch 78/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.6702\n",
      "Epoch 79/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.5851\n",
      "Epoch 80/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.6146\n",
      "Epoch 81/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.5360\n",
      "Epoch 82/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4758\n",
      "Epoch 83/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.5314\n",
      "Epoch 84/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4373\n",
      "Epoch 85/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.5097\n",
      "Epoch 86/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.5093\n",
      "Epoch 87/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4650\n",
      "Epoch 88/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4476\n",
      "Epoch 89/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4491\n",
      "Epoch 90/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4835\n",
      "Epoch 91/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.5015\n",
      "Epoch 92/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3918\n",
      "Epoch 93/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4150\n",
      "Epoch 94/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4293\n",
      "Epoch 95/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4129\n",
      "Epoch 96/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3842\n",
      "Epoch 97/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3805\n",
      "Epoch 98/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3520\n",
      "Epoch 99/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3494\n",
      "Epoch 100/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3305\n",
      "Epoch 101/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4025\n",
      "Epoch 102/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3083\n",
      "Epoch 103/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3381\n",
      "Epoch 104/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3563\n",
      "Epoch 105/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3725\n",
      "Epoch 106/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2917\n",
      "Epoch 107/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3206\n",
      "Epoch 108/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2918\n",
      "Epoch 109/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3214\n",
      "Epoch 110/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2537\n",
      "Epoch 111/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2744\n",
      "Epoch 112/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3315\n",
      "Epoch 113/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2693\n",
      "Epoch 114/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2942\n",
      "Epoch 115/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3194\n",
      "Epoch 116/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3509\n",
      "Epoch 117/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3142\n",
      "Epoch 118/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2908\n",
      "Epoch 119/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2555\n",
      "Epoch 120/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3361\n",
      "Epoch 121/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2403\n",
      "Epoch 122/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2308\n",
      "Epoch 123/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2335\n",
      "Epoch 124/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2275\n",
      "Epoch 125/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2392\n",
      "Epoch 126/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2420\n",
      "Epoch 127/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1853\n",
      "Epoch 128/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2242\n",
      "Epoch 129/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2453\n",
      "Epoch 130/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2416\n",
      "Epoch 131/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2267\n",
      "Epoch 132/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2384\n",
      "Epoch 133/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2892\n",
      "Epoch 134/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2351\n",
      "Epoch 135/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2142\n",
      "Epoch 136/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1680\n",
      "Epoch 137/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1731\n",
      "Epoch 138/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.3551\n",
      "Epoch 139/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2420\n",
      "Epoch 140/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1920\n",
      "Epoch 141/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2799\n",
      "Epoch 142/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2041\n",
      "Epoch 143/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1550\n",
      "Epoch 144/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2268\n",
      "Epoch 145/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1731\n",
      "Epoch 146/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2051\n",
      "Epoch 147/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2041\n",
      "Epoch 148/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1978\n",
      "Epoch 149/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1876\n",
      "Epoch 150/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1943\n",
      "Epoch 151/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1562\n",
      "Epoch 152/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1711\n",
      "Epoch 153/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1691\n",
      "Epoch 154/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2297\n",
      "Epoch 155/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2334\n",
      "Epoch 156/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1878\n",
      "Epoch 157/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1553\n",
      "Epoch 158/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4258\n",
      "Epoch 159/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1748\n",
      "Epoch 160/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2575\n",
      "Epoch 161/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1988\n",
      "Epoch 162/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2199\n",
      "Epoch 163/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2014\n",
      "Epoch 164/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1575\n",
      "Epoch 165/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1580\n",
      "Epoch 166/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1396\n",
      "Epoch 167/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1756\n",
      "Epoch 168/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1149\n",
      "Epoch 169/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1640\n",
      "Epoch 170/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1518\n",
      "Epoch 171/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1381\n",
      "Epoch 172/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2008\n",
      "Epoch 173/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1840\n",
      "Epoch 174/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1454\n",
      "Epoch 175/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1718\n",
      "Epoch 176/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1792\n",
      "Epoch 177/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1587\n",
      "Epoch 178/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1369\n",
      "Epoch 179/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2102\n",
      "Epoch 180/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.4126\n",
      "Epoch 181/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1487\n",
      "Epoch 182/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1226\n",
      "Epoch 183/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2081\n",
      "Epoch 184/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1083\n",
      "Epoch 185/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.2121\n",
      "Epoch 186/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1482\n",
      "Epoch 187/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1385\n",
      "Epoch 188/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1519\n",
      "Epoch 189/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1339\n",
      "Epoch 190/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1235\n",
      "Epoch 191/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1348\n",
      "Epoch 192/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1497\n",
      "Epoch 193/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1376\n",
      "Epoch 194/300\n",
      "36/36 [==============================] - 257s 7s/step - loss: 0.1273\n",
      "Epoch 195/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1083\n",
      "Epoch 196/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1379\n",
      "Epoch 197/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1516\n",
      "Epoch 198/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1734\n",
      "Epoch 199/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1252\n",
      "Epoch 200/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1218\n",
      "Epoch 201/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1396\n",
      "Epoch 202/300\n",
      "36/36 [==============================] - 256s 7s/step - loss: 0.1287\n",
      "Epoch 203/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1346\n",
      "Epoch 204/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1623\n",
      "Epoch 205/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1276\n",
      "Epoch 206/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1145\n",
      "Epoch 207/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1240\n",
      "Epoch 208/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1263\n",
      "Epoch 209/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1155\n",
      "Epoch 210/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1842\n",
      "Epoch 211/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1602\n",
      "Epoch 212/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1193\n",
      "Epoch 213/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1498\n",
      "Epoch 214/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1446\n",
      "Epoch 215/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1441\n",
      "Epoch 216/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1199\n",
      "Epoch 217/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1055\n",
      "Epoch 218/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1232\n",
      "Epoch 219/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.2139\n",
      "Epoch 220/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1301\n",
      "Epoch 221/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1772\n",
      "Epoch 222/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1799\n",
      "Epoch 223/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1105\n",
      "Epoch 224/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1337\n",
      "Epoch 225/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1486\n",
      "Epoch 226/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1412\n",
      "Epoch 227/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1228\n",
      "Epoch 228/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1489\n",
      "Epoch 229/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1378\n",
      "Epoch 230/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1416\n",
      "Epoch 231/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1135\n",
      "Epoch 232/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1319\n",
      "Epoch 233/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1284\n",
      "Epoch 234/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1362\n",
      "Epoch 235/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1570\n",
      "Epoch 236/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1320\n",
      "Epoch 237/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1866\n",
      "Epoch 238/300\n",
      "36/36 [==============================] - 258s 7s/step - loss: 0.1011\n",
      "Epoch 239/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1217\n",
      "Epoch 240/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1385\n",
      "Epoch 241/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.2037\n",
      "Epoch 242/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1239\n",
      "Epoch 243/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0987\n",
      "Epoch 244/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1055\n",
      "Epoch 245/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1223\n",
      "Epoch 246/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1260\n",
      "Epoch 247/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1171\n",
      "Epoch 248/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0705\n",
      "Epoch 249/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1056\n",
      "Epoch 250/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1682\n",
      "Epoch 251/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1337\n",
      "Epoch 252/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1643\n",
      "Epoch 253/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0852\n",
      "Epoch 254/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1039\n",
      "Epoch 255/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1239\n",
      "Epoch 256/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1930\n",
      "Epoch 257/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1172\n",
      "Epoch 258/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1058\n",
      "Epoch 259/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1182\n",
      "Epoch 260/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0913\n",
      "Epoch 261/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1238\n",
      "Epoch 262/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1374\n",
      "Epoch 263/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0980\n",
      "Epoch 264/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1254\n",
      "Epoch 265/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0752\n",
      "Epoch 266/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0873\n",
      "Epoch 267/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1184\n",
      "Epoch 268/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1195\n",
      "Epoch 269/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0910\n",
      "Epoch 270/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1024\n",
      "Epoch 271/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0872\n",
      "Epoch 272/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1752\n",
      "Epoch 273/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1191\n",
      "Epoch 274/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0890\n",
      "Epoch 275/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1204\n",
      "Epoch 276/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1135\n",
      "Epoch 277/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0993\n",
      "Epoch 278/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0769\n",
      "Epoch 279/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1251\n",
      "Epoch 280/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1416\n",
      "Epoch 281/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0790\n",
      "Epoch 282/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1082\n",
      "Epoch 283/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1071\n",
      "Epoch 284/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1396\n",
      "Epoch 285/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1113\n",
      "Epoch 286/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1446\n",
      "Epoch 287/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0894\n",
      "Epoch 288/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1568\n",
      "Epoch 289/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0778\n",
      "Epoch 290/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0875\n",
      "Epoch 291/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0968\n",
      "Epoch 292/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1090\n",
      "Epoch 293/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1171\n",
      "Epoch 294/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1165\n",
      "Epoch 295/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1030\n",
      "Epoch 296/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0788\n",
      "Epoch 297/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.0756\n",
      "Epoch 298/300\n",
      "36/36 [==============================] - 259s 7s/step - loss: 0.1013\n",
      "Epoch 299/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.0843\n",
      "Epoch 300/300\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.1122\n",
      "teste com dataset variacao de treino\n",
      "abril.mp4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m cont \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_variation):\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;66;03m# x_test = test.reshape(-1, 32, 32, 1)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m files[np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray([test]), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))] \u001b[38;5;241m==\u001b[39m \u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     26\u001b[0m     cont \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(files[idx])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "estop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=1\n",
    ")\n",
    "\n",
    "Y_variation = to_categorical(Y_variation)\n",
    "print(Y_variation.shape)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/train_cnnXrnn_baseline\")\n",
    "#treino com captura pro tensorboard, nao utilizei o estop pois parecia ter melhores resultados sem ele\n",
    "#treino com o dataset apenas com preprocess\n",
    "model.fit(X_variation, Y_variation, epochs=300,\n",
    "          # validation_data=(X_variation, Y),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "61qTsTYQPGK4"
   },
   "outputs": [],
   "source": [
    "model.save('ariel-variationtrain.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teste com dataset original\n",
      "abril.mp4\n",
      "acesso.mp4\n",
      "acima.mp4\n",
      "acordo.mp4\n",
      "agora.mp4\n",
      "agosto.mp4\n",
      "ainda.mp4\n",
      "alta.mp4\n",
      "altura.mp4\n",
      "ambos.mp4\n",
      "americano.mp4\n",
      "and.mp4\n",
      "ano.mp4\n",
      "anos.mp4\n",
      "anterior.mp4\n",
      "antes.mp4\n",
      "apenas.mp4\n",
      "apresenta.mp4\n",
      "assim.mp4\n",
      "ator.mp4\n",
      "atual.mp4\n",
      "autor.mp4\n",
      "banda.mp4\n",
      "bem.mp4\n",
      "brasileiro.mp4\n",
      "campanha.mp4\n",
      "campo.mp4\n",
      "capital.mp4\n",
      "casa.mp4\n",
      "casamento.mp4\n",
      "categoria.mp4\n",
      "centro.mp4\n",
      "cerca.mp4\n",
      "cinco.mp4\n",
      "cinema.mp4\n",
      "classe.mp4\n",
      "clube.mp4\n",
      "comunidade.mp4\n",
      "conta.mp4\n",
      "contra.mp4\n",
      "contrato.mp4\n",
      "controle.mp4\n",
      "corpo.mp4\n",
      "data.mp4\n",
      "dentro.mp4\n",
      "deu.mp4\n",
      "deve.mp4\n",
      "devido.mp4\n",
      "dez.mp4\n",
      "dezembro.mp4\n",
      "dia.mp4\n",
      "direito.mp4\n",
      "disco.mp4\n",
      "disso.mp4\n",
      "distrito.mp4\n",
      "dois.mp4\n",
      "embora.mp4\n",
      "empresa.mp4\n",
      "equipe.mp4\n",
      "escola.mp4\n",
      "especial.mp4\n",
      "esposa.mp4\n",
      "estado.mp4\n",
      "estudos.mp4\n",
      "exemplo.mp4\n",
      "faixa.mp4\n",
      "fazer.mp4\n",
      "fevereiro.mp4\n",
      "fez.mp4\n",
      "filme.mp4\n",
      "fim.mp4\n",
      "final.mp4\n",
      "forma.mp4\n",
      "forte.mp4\n",
      "frente.mp4\n",
      "futebol.mp4\n",
      "geral.mp4\n",
      "governo.mp4\n",
      "grande.mp4\n",
      "grupo.mp4\n",
      "guerra.mp4\n",
      "hoje.mp4\n",
      "homem.mp4\n",
      "idade.mp4\n",
      "igreja.mp4\n",
      "ilha.mp4\n",
      "in.mp4\n",
      "interior.mp4\n",
      "internacional.mp4\n",
      "jovem.mp4\n",
      "julho.mp4\n",
      "junho.mp4\n",
      "lado.mp4\n",
      "lei.mp4\n",
      "lista.mp4\n",
      "livro.mp4\n",
      "local.mp4\n",
      "luta.mp4\n",
      "maio.mp4\n",
      "maior.mp4\n",
      "maioria.mp4\n",
      "mar.mp4\n",
      "marca.mp4\n",
      "meio.mp4\n",
      "membro.mp4\n",
      "menos.mp4\n",
      "mercado.mp4\n",
      "mil.mp4\n",
      "militar.mp4\n",
      "modelo.mp4\n",
      "morte.mp4\n",
      "movimento.mp4\n",
      "mulher.mp4\n",
      "mundo.mp4\n",
      "nacional.mp4\n",
      "nome.mp4\n",
      "nomeado.mp4\n",
      "nova.mp4\n",
      "novembro.mp4\n",
      "novo.mp4\n",
      "nunca.mp4\n",
      "objetivo.mp4\n",
      "obra.mp4\n",
      "oeste.mp4\n",
      "of.mp4\n",
      "oficial.mp4\n",
      "oito.mp4\n",
      "onde.mp4\n",
      "ordem.mp4\n",
      "origem.mp4\n",
      "original.mp4\n",
      "ouro.mp4\n",
      "papel.mp4\n",
      "personagem.mp4\n",
      "pessoa.mp4\n",
      "pode.mp4\n",
      "presente.mp4\n",
      "primeira.mp4\n",
      "primeiro.mp4\n",
      "principal.mp4\n",
      "professor.mp4\n",
      "profissional.mp4\n",
      "programa.mp4\n",
      "projeto.mp4\n",
      "qualquer.mp4\n",
      "quanto.mp4\n",
      "quase.mp4\n",
      "real.mp4\n",
      "rede.mp4\n",
      "redor.mp4\n",
      "rei.mp4\n",
      "representa.mp4\n",
      "resultado.mp4\n",
      "revista.mp4\n",
      "rio.mp4\n",
      "sede.mp4\n",
      "seguinte.mp4\n",
      "segunda.mp4\n",
      "segundo.mp4\n",
      "seis.mp4\n",
      "semana.mp4\n",
      "sempre.mp4\n",
      "sete.mp4\n",
      "setembro.mp4\n",
      "sob.mp4\n",
      "sobre.mp4\n",
      "social.mp4\n",
      "sociedade.mp4\n",
      "somente.mp4\n",
      "sucesso.mp4\n",
      "sul.mp4\n",
      "tal.mp4\n",
      "tanto.mp4\n",
      "tarde.mp4\n",
      "tempo.mp4\n",
      "teoria.mp4\n",
      "ter.mp4\n",
      "terceiro.mp4\n",
      "termo.mp4\n",
      "terra.mp4\n",
      "tipo.mp4\n",
      "todo.mp4\n",
      "torneio.mp4\n",
      "total.mp4\n",
      "usado.mp4\n",
      "valor.mp4\n",
      "vez.mp4\n",
      "vida.mp4\n",
      "volta.mp4\n",
      "o modelo teve 82.89473684210526% de acertos\n"
     ]
    }
   ],
   "source": [
    "print('teste com dataset original')\n",
    "cont = 0\n",
    "for idx, test in enumerate(data):\n",
    "  # x_test = test.reshape(-1, 32, 32, 1)\n",
    "  if files[np.argmax(model.predict(np.array([test]), verbose=0))] == files[idx]:\n",
    "    cont += 1\n",
    "    print(files[idx])\n",
    "\n",
    "print(f\"o modelo teve {(cont/len(files))*100}% de acertos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm-iPM7of1LK"
   },
   "outputs": [],
   "source": [
    "# tensorboard = TensorBoard(log_dir=\"logs/train_cnnXrnn_baseline_mp\")\n",
    "# model.fit(X_mp, Y, epochs=1000,\n",
    "#           # validation_data=(train_generator_mp),\n",
    "#           # callbacks=[tensorboard, estop])\n",
    "#           callbacks=[tensorboard, estop])\n",
    "# #treino com o dataset com preprocess e mediapipe extractor\n",
    "\n",
    "\n",
    "# for idx, test in enumerate(mp_data):\n",
    "#   # x_test = test.reshape(-1, 32, 32, 1)\n",
    "#   if files[np.argmax(model.predict(np.array([test]), verbose=0))] == files[idx]:\n",
    "#     print(files[idx])\n",
    "model.save('ariel-mp.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JPZF0zpf4o1"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
